Up to this point, we have explored some of the insights that make our tool
efficient. Now, we compare with existing tools that do not take a
component-wise view, and thus cannot exploit compositionality as we can,
instead, computing global statespaces, or using unfoldings or other
partial-order reduction techniques to avoid the statespace explosion problem.
Essentially, we would like to show that while compositionality is a theoretical
nicety, it also has significant impact on performance in practice.  Therefore,
in this section, we quantitatively compare our approach with existing
state-of-the-art tools. First, we introduce the tools we are comparing against,
before discussing our testing platform and methodology. After presenting the
results we interpret them, discussing some key points.

\subsection{Related Tools}

\newcommand{\footnoteUrl}[1]{\footnote{\url{#1}}}

To evaluate the performance of \penrose{}, it was compared with five tools that
make up part of the current state-of-the-art:
\begin{enumerate}
    \def\urlLOLA{\footnoteUrl{http://download.gna.org/service-tech/lola/}}
    \def\urlPUNF{\footnoteUrl{http://homepages.cs.ncl.ac.uk/victor.khomenko/tools/punf/}}
    \def\urlCLP{\footnoteUrl{http://homepages.cs.ncl.ac.uk/victor.khomenko/tools/clp/}}
    \def\urlCUNF{\label{fn:CUNF}\footnoteUrl{http://code.google.com/p/cunf/}}
    \def\urlCNA{\footnote{Available packaged with CUNF}}
    \def\urlMARCIE{\footnoteUrl{http://www-dssz.informatik.tu-cottbus.de/DSSZ/Software/Marcie}}
    \def\urlTAPAAL{\footnoteUrl{http://www.tapaal.net/}}
    \item LOLA\urlLOLA~\cite{Schmidt2000b},
    \item The PUNF\urlPUNF{} unfolder with CLP\urlCLP{} checker,
    \item The CUNF\urlCUNF~\cite{Rodriguez2013a} unfolder, and CNA\urlCNA
        checker,
    \item MARCIE\urlMARCIE~\cite{Heiner2013},
    \item TAPAAL\urlTAPAAL.
\end{enumerate}

Several of these tools are \emph{the} current leading choices of tools for
Petri net reachability checking and state-space generation. Indeed, in the
recent Petri net model-checking competition (MCC'14)~\cite{MCC2014}, LOLA won,
and TAPAAL was second in the reachability category, whilst in the state-space
generation category MARCIE won, with TAPAAL third. The two other tools, CLP and
CNA are both comprised of two distinct tools, one to pre-process (unfold) the
input net, and the other to check the required property on the unfolding. CLP's
unfolder, PUNF, uses parallelisation techniques from~\cite{Heljanko2002}, while
CLP itself uses linear-programming techniques from~\cite{Koutny2000}. CNA is
included since it handles contextual nets (i.e. those with read arcs), and thus
is able, like \penrose{}, to calculate results for the \overtakeSys{-} and
\counterSys{-} examples.

The tested tools employ different techniques, as reported by the tools'
corresponding entries in MCC'14:
\begin{itemize}
\item LOLA: stubborn sets, symmetry reduction, unfolding,
\item PUNF/CLP: parallelisation, linear programming,
\item CUNF/CNA: unfolding, SAT/SMT,
\item MARCIE: decision diagrams,
\item TAPAAL: symmetry reduction.
\end{itemize}

For overviews of these techniques, refer back to~\secref{sec:related}.

\subsection{Testing Platform}

All experiments were run on two Ubuntu Linux virtual machines (both with 4GB
RAM and a 4-core CPU: one a 32-bit and the other a 64-bit CPU), hosted on an
Intel i5-2450 2.50GHz CPU, 8GB of RAM, running 64-bit Ubuntu Linux. CLP
required a 32-bit platform, and thus virtual machines were used to give a
common-specification platform.

For MARCIE and TAPAAL, we used the virtual machine images provided for the 2014
Petri net Model Checking Competition (MCC 2014)~\cite{MCC2014}, available at
\url{http://mcc.lip6.fr/2014/results.php}. For CUNF, CNA, PUNF and CLP we used
pre-compiled binaries from the corresponding authors' websites, and compiled
LOLA from source.

\subsection{Testing Methodology}

Tool performance was recorded using the standard Unix \texttt{time} command,
measuring total (wall-clock) time and peak memory usage\footnote{Specifically,
using the command \texttt{/usr/bin/time -f `\%E \%M' CMD}.}. For each tool and
example configuration (i.e. example and size parameter(s)) we took the mean of
5 runs, obtaining the average time and memory consumption.

While \penrose{} directly computes using the particular wiring decomposition
that specifies each problem; all other tools were provided input that was
generated by first computing the composite net and then converting into
suitable format\footnote{Either LL\_NET format or LOLA's input format.}. The
time taken for this conversion was not included in the performance
benchmarking---only the processing time of the individual tools was recorded.

\subsection{Testing Results}

In the following results tables we use the key:

\makebox[\textwidth][c]{
\begin{tabular}{r c l}
    \emph{\timeoutResult}   & $\implies$ & time-out (300 seconds) \\
    \emph{\oomResult}       & $\implies$ & memory-exhaustion (4GB) \\
    \emph{\incorrectResult} & $\implies$ & incorrect result \\
    \emph{\queryPortsUnhandledResult} & $\implies$ & example skipped due to
        lack of read arc support \\
\end{tabular}
}

The best performance for each problem instance is highlighted. We present four
tables, two recording total time spent, and two recording total memory usage.
The tables are split not only for space reasons, but also to differentiate
clearly between examples where \penrose{} exhibits scalable performance
relative to the competition, and those where it exhibits poor performance. The
timing results are presented in Tables~\ref{tab:time1} and~\ref{tab:time2} and
the memory results in Tables~\ref{tab:mem1} and~\ref{tab:mem2}.

\begin{table}
\caption{Time results}
\label{tab:time1}
\centering
\resizebox{0.95\linewidth}{!}{%
\input{results_time_1}
}
\end{table}

\begin{table}
\caption{Time results continued}
\label{tab:time2}
\centering
\resizebox{0.95\linewidth}{!}{%
\input{results_time_2}
}
\end{table}

\begin{table}
\caption{Memory results}
\label{tab:mem1}
\centering
\resizebox{0.95\linewidth}{!}{%
\input{results_memory_1}
}
\end{table}

\begin{table}
\caption{Memory results continued}
\label{tab:mem2}
\centering
\resizebox{0.95\linewidth}{!}{%
\input{results_memory_2}
}
\end{table}

\subsection{Discussion}

For the systems with scalable performance, \penrose{} is able to check
reachability in around 100 milliseconds, even for large parameter-sizes.
Similarly, the memory use of \penrose{} is always less than 10MB. A general
observation is that other tools do not perform as well for anything other than
small parameter sizes. Indeed, the simplest system, \bufferSys{-} is only
checkable by \penrose{}, LOLA and MARCIE at parameter 512, and only \penrose{}
for larger parameters, despite a simple structure and marking.

Since CNA can only check coverability (i.e. it cannot specify that particular
places should \emph{not} be marked), we must trivially adapt the \forkC{}
component of \diningphilosophersSys{-}, such that it has two places: one is
marked when the fork is present and the other when it is absent. Thus we can
specify all forks being taken with the positive marking of all ``taken''
places, rather than the absence of the token in the original ``status'' place.

\iteratedchoiceSys{-} is an example given by {Khomenko et
al.}~\cite{Khomenko2005} that is shown to have an exponential unfolding;
precisely, \iteratedchoiceSys{\aN} has an unfolding with $2^{\aN + 1} - 1$
places. Due to this exponentially-sized unfolding, the results show that
moderately-sized instances cannot be handled by the tested tools. \penrose{},
on the other hand, is able to handle very large instances quickly. In the
originating paper (\cite{Khomenko2005}), an alternative structure, Merged
Processes (MPs), were introduced to avoid such exponential unfoldings, however,
we are not aware of a model checker that implements MPs.

Some proportion of the overhead encountered by other tools in large examples
can be attributed to the size of the input file that must be processed (e.g.
the ll\_net format is $\approx500$ KB for \overtakeSys{512} and LOLA's input is
$\approx1.3$ MB for \DACSys{4096}). Indeed, this is part of the point of this
thesis: using component-wise specification and \DSL{}, we can specify certain
systems in a vastly more concise way.

Furthermore, since we interpret the benchmark systems as 1-bounded (recall that
PNBs use elementary-net firing semantics) and that the tested tools do not give
the ability to restrict place token bounds, is likely that a large proportion
of the time spent is wasted exploring unnecessary states (where places have >1
token).

An observation regarding \penrose{}'s performance is that certain markings are
certainly going to be much faster to check for a given system than others; a
simple example being \distree{-}{-}. The tested marking requires a token in
\emph{every} leaf place, given a single starting token in the root place.
\penrose{} is extremely slow to check this reachability problem, due to the
statespace explosion problem: large intermediate \TNFA{}s are constructed, as
discussed at the end of \chpref{chp:improveEfficiency}. Indeed, \penrose{}
calculates the intermediate results that record \emph{all} possible ways to
introduce enough tokens into the sub-systems, to reach the desired marking.
However, it is only at the final composition step that \penrose{} discovers
that there will only ever be a single token in the system, and thus all
intermediate results are discarded. Indeed, changing to the target marking that
requires a token in \emph{any one} leaf place \emph{is} fast to check. Observe
that \penrose{} currently implements a strict evaluation strategy, sub-system
reachability \TNFA{}s are calculated, without them necessarily being required.
Indeed, this is necessary for the current memoisation implementation --- to
know if we have seen a particular \TNFA{} composition before, we must fully
evaluate each of the arguments. Future work is to explore the possibility of
using a lazy evaluation strategy, whereby evaluation is only performed on
demand --- in the poorly performing example, one could imagine only evaluating
two far enough to observe that the sub-system requires at least two tokens to
reach its target marking, which cannot be provided by the root's single token,
thus the global marking is not reachable.

Similarly, \hartstoneSys{-} and \tokenringSys{-} (which are fundamentally quite
similar), contain sub-nets that can store $n$ tokens, thus the \TNFA{} must
represent that tokens can be (arbitrarily) added or removed. In fact, only one
``token'' is ever inserted/removed into the system. Future work will
investigate the applicability of techniques such as Larsen's relativized
bisimulation~\cite{Larsen1987}, that control the allowable interactions with an
environment (in this case, ensuring at most one token is taken from the
context).

Our final comments are to draw attention to the fact that tools based on
unfolding or other partial-order reduction are mature, with years of
development, and established tools such as LOLA have been highly optimised.
\penrose{} is implemented in the high level functional language Haskell, and
further, has had little to no optimisation effort applied --- it is a
proof-of-concept.  Despite this, it is able to out-perform the tools in several
examples, as demonstrated by our results. It could be argued that the playing
field is unfair: \penrose{} uses a formal description of the decomposition of a
problem at hand into smaller components while other tools take a global,
monolithic net as input. This is, however, precisely our point: there is no
reason for model checkers not to take advantage of compositional
descriptions --- it is how real systems are specified/designed.

We see significant room for future improvement in \penrose{}; indeed, our
approach makes no use of the partial-order reduction techniques of other tools,
which we conjecture are orthogonal to and therefore compatible with our
compositional approach.
